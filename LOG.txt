Week 45:
Thursday: 
    Read introductory paper (ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models).
    Kick off meeting. 

Friday: 
    Set up of git and slack for communication. Started reading follow up paper (A review of uncertainty 
    quantification in deep learning: Techniques applications and challenges). Created report template. Started 
    creating presentation regarding summary of the introductory papers for meeting next Wednesday.

Week 46:
Monday: 
    Finished the presentation summarizing the papers. Started structuring what to mention in what sections in the report.

Tuesday: 
    Started writing introduction and theory in the report. 

Wednesday:	
    Meeting with Prashant where we presented the two papers mentioned above and discussed what we want to research more about.
    Need to decide if we e.g. want to do benchmark testing or/and see if we can both find the epistemic uncertainty from the deterministic 
    embeddings and also find the aleatoric uncertainty from the data and in which field we want to focus, continue on CLIP or try to find 
    uncertainties in e.g. healthcare. 

Thursday:
    Continue reading on related papers regarding choice of adapter. Looking into how to 
    separate aleatoric and epistemic uncertainties. Looking in to the Bayes By Backprop (BBB)
    adapter and how it could be implemented for the CLIP model. Looking into the ProbVLM code
    to understand how it works. Preparing questions and suggestions on how to move the project
    forward for a meeting with Prashant and Li tomorrow.

Friday:
    Meeting with Prashant and Li. Discussing how aleatoric and epistemic uncertainties work and 
    can be separated. Discussing the choice of adapter. Decided that we should start by implementing 
    the BBB BN. A second step would be to implement a separate term for the aleatoric uncertainty
    if the ProbVLM approach seems inadequate. Further down the line a benchmark against the ProbVLM
    approach would be implemented and visualizations with attention instead of diffusion.

    Preparing the Alvis environment and begin looking into how to implement the BBB. Looking into
    Bayesian Network layers using PyTorch.

Week 47:
Monday: 
    Preparing the alvis environment. Configuring VPN, and getting familiar with alvis.  
    Implementing a simple BNN as a proof of concept.

Tuesday:
    Downloading the COCO datasets to train the CLIP and ProbVLM model. Configuring the file structures and the datasets.
    Preparing a script that can be run as a batch job.
    Since downloading, extracting and moving the data to the cluster takes a long time we decided to write on the report
    and try submitting the batch job tomrorrow, so the data can load over the night.

Wednesday:
    Managed to get the whole ProbVLM pipeline to work on alvis (alot of debugging). 
    Tried training and evaluating a simple BNN which is working. 
    Downloaded all of the other datasets to evaluate uncertainties iid and ood, i.e. train on one of the dataset e.g. COCO and test it on the others.  

Thursday:
    Meeting with Prashant and collegues. Discussed our implementation, choice of loss function.
    Discussing next steps. The next steps are to train the entire ProbVLM and our BBB model on alvis.
    We are first gonna implement early stopping, adaptive learning rate, change the priors according
    to documentation. Then we are gonna train both models and time them (that is important so we can
    compare the computational complexities of both models, ours should be much faster than ProbVLM
    since they are utilizing drop-out which is a quite expensive operation). 

    We also discussed how we should evaluate our model. One thing to just sanity check is to check if
    out of distribution samples have higher uncertainties than in distribution samples. We could 
    also produce the roc recall-uncertainty curves to compare with probvlm. For the real evaluation
    we discussed using a downstream task such as captioning, or using guided attention, so we can 
    compare the methods on something real.


Friday:


Week 48:
    To do list: 
    	1. Investigate sigma prior and mu prior
	2. Investigate adaptive learning rate (problems with MSE jumping)
	3. Rerun uncertainty estimate for when the new BBB algo is done
	4. PCA to visualize the embedding space
	5. Produce ROC-curves
	6. Downstream task

    Questions for Prashant on Friday:
    	1. Good mu and sigma prior?
	2. Discuss the BNN architecture (is 3 layers enough). Should we use BL everywhere?
	3. Discuss adaptive learning rate and early stopping (if we want to compare time)
		also ProbVLM doesn't use adaptive learning rate and they have fixed epochs
		(though they have an adaptive learner but it steps outside the for loop????)
	4. When we inspect uncertainty from multiple forward pass some of the values are very 
		large -> infinite epistemic uncertainty (even for ProbVLM)
		From debugging: GG_uncer is the one responsible for the large values. If we remove them we get this for probvlm:
			coco: 3193504:      Avg. Image Uncertainty: 0.0033659490291029215, Avg. Text Uncertainty: 1.9942842754971934e-07
			flickr: 3193512:    Avg. Image Uncertainty: 0.0045932624489068985, Avg. Text Uncertainty: 3.777297763463139e-07
		which makes sense (higher for ood)
	5. Downstream task
	6. The loss for ProbVLM is 2 from start and ends at 1.5, BBB is 10e8 at start and ends at 17
	7. Bayesian Linear layer in Encoder and Decoder of adapter or just in Encoder since dropout is only in Encoder part?
	8. Why is there no ReLU after the mu block, is there a reason or did they make a mistake?
	9. get_GGuncer please help
	10. ProbVLM use a determined LR, should we also or should we go adaptive?
	11. Why did BBB become so good when we remove the bayesian layers from the decoder

Monday:
	Training ProbVLM and BBB (with the additional KL term in the loss).
	Setting up the flickr dataset, and evaluating it. Writing a script to extract the epistemic uncertainty in order to compare the uncertainty
	for coco vs flickr (coco should be lower since it is trained on it).
	Trying to debug the recall script (something is still wrong).
 
Tuesday:
	Debugging what is going wrong with the models and evaluation script. Checking the feature vectors of the samples using PCA. The trained BBB
	seemed to collapse in the center while the ProbVLM looked better after the dimensionality reduction using PCA. So something went wrong with the 
	BBB training which we are also trying to debug. It may be the prior of mu and sigma or that we did not include the KL divergence term at first. 
	Retraining the model is ongoing.     
